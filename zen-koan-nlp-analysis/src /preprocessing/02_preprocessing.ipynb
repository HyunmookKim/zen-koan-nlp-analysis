{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM18OjwHo9+vFNBGXZ6CuBu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyunmookKim/zen-koan-nlp-analysis/blob/master/zen-koan-nlp-analysis/src%20/preprocessing/02_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ì„  ê³µì•ˆ NLP ì—°êµ¬ - ì „ì²˜ë¦¬ Step 2.1: ê³µì•ˆ\n",
        "1) Blue Cliff Case 100 ìˆ˜ì •\n",
        "2) Mumonkan 48 + Blue Cliff 100 íŒŒì‹±\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 1: í™˜ê²½ ì„¤ì •\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/zen-koan-nlp-analysis'\n",
        "RAW_PATH = f'{BASE_PATH}/data/extracted_texts_v10'\n",
        "OUTPUT_PATH = f'{BASE_PATH}/data/processed'\n",
        "CHUNKS_PATH = f'{BASE_PATH}/data/chunks'\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(CHUNKS_PATH, exist_ok=True)\n",
        "\n",
        "print(\"í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 2: Blue Cliff Case 100 ìˆ˜ì •\n",
        "# =============================================================================\n",
        "\n",
        "filepath = f\"{RAW_PATH}/koans_blue_cliff.txt\"\n",
        "\n",
        "with open(filepath, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# ìˆ˜ì •: === CASE === â†’ === CASE 100 ===\n",
        "if '=== CASE ===' in content:\n",
        "    content = content.replace('=== CASE ===', '=== CASE 100 ===')\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "    print(\"âœ… Blue Cliff Case 100 ìˆ˜ì • ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\"â„¹ï¸ ì´ë¯¸ ìˆ˜ì •ë˜ì–´ ìˆìŒ\")\n",
        "\n",
        "# í™•ì¸\n",
        "markers = re.findall(r'=== CASE (\\d+) ===', content)\n",
        "print(f\"Blue Cliff ì´ ì¼€ì´ìŠ¤: {len(markers)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 3: spaCy ì„¤ì •\n",
        "# =============================================================================\n",
        "\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.max_length = 3000000\n",
        "\n",
        "print(\"spaCy ë¡œë“œ ì™„ë£Œ!\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 4: ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
        "# =============================================================================\n",
        "\n",
        "def load_text_file(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    metadata = {}\n",
        "    lines = content.split('\\n')\n",
        "    body_start = 0\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.startswith('#'):\n",
        "            if ':' in line:\n",
        "                key, value = line[1:].split(':', 1)\n",
        "                metadata[key.strip().lower()] = value.strip()\n",
        "            body_start = i + 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    body = '\\n'.join(lines[body_start:]).strip()\n",
        "    return body, metadata\n",
        "\n",
        "def tokenize_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [t for t in doc if not t.is_space]\n",
        "    sentences = list(doc.sents)\n",
        "    return len(tokens), len(sentences)\n",
        "\n",
        "print(\"í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5: ê³µì•ˆ íŒŒì‹±\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"ê³µì•ˆ íŒŒì‹± ì‹œì‘\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "koan_units = []\n",
        "koan_full_text = \"\"\n",
        "case_pattern = r'=== CASE (\\d+) ==='\n",
        "\n",
        "# --- Mumonkan ---\n",
        "filepath = f\"{RAW_PATH}/koans_mumonkan.txt\"\n",
        "text, metadata = load_text_file(filepath)\n",
        "\n",
        "markers = list(re.finditer(case_pattern, text))\n",
        "print(f\"\\nMumonkan ë§ˆì»¤ ìˆ˜: {len(markers)}\")\n",
        "\n",
        "for i, match in enumerate(markers):\n",
        "    case_num = int(match.group(1))\n",
        "    start = match.end()\n",
        "    end = markers[i+1].start() if i+1 < len(markers) else len(text)\n",
        "    case_text = text[start:end].strip()\n",
        "\n",
        "    if case_text:\n",
        "        koan_full_text += case_text + \"\\n\\n\"\n",
        "        token_count, sent_count = tokenize_text(case_text)\n",
        "        koan_units.append({\n",
        "            'category': 'koan',\n",
        "            'text_name': 'mumonkan',\n",
        "            'unit_id': f'mumonkan_case_{case_num}',\n",
        "            'unit_num': case_num,\n",
        "            'text': case_text,\n",
        "            'token_count': token_count,\n",
        "            'sentence_count': sent_count,\n",
        "            'translator': 'Robert Aitken'\n",
        "        })\n",
        "\n",
        "mumonkan_count = len([u for u in koan_units if u['text_name'] == 'mumonkan'])\n",
        "print(f\"Mumonkan íŒŒì‹± ì™„ë£Œ: {mumonkan_count} cases\")\n",
        "\n",
        "# --- Blue Cliff Record ---\n",
        "filepath = f\"{RAW_PATH}/koans_blue_cliff.txt\"\n",
        "text, metadata = load_text_file(filepath)\n",
        "\n",
        "markers = list(re.finditer(case_pattern, text))\n",
        "print(f\"\\nBlue Cliff ë§ˆì»¤ ìˆ˜: {len(markers)}\")\n",
        "\n",
        "for i, match in enumerate(markers):\n",
        "    case_num = int(match.group(1))\n",
        "    start = match.end()\n",
        "    end = markers[i+1].start() if i+1 < len(markers) else len(text)\n",
        "    case_text = text[start:end].strip()\n",
        "\n",
        "    if case_text:\n",
        "        koan_full_text += case_text + \"\\n\\n\"\n",
        "        token_count, sent_count = tokenize_text(case_text)\n",
        "        koan_units.append({\n",
        "            'category': 'koan',\n",
        "            'text_name': 'blue_cliff',\n",
        "            'unit_id': f'blue_cliff_case_{case_num}',\n",
        "            'unit_num': case_num,\n",
        "            'text': case_text,\n",
        "            'token_count': token_count,\n",
        "            'sentence_count': sent_count,\n",
        "            'translator': 'Thomas Cleary'\n",
        "        })\n",
        "\n",
        "blue_cliff_count = len([u for u in koan_units if u['text_name'] == 'blue_cliff'])\n",
        "print(f\"Blue Cliff íŒŒì‹± ì™„ë£Œ: {blue_cliff_count} cases\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 6: Level 1 ì €ì¥\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Level 1 ì €ì¥\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "df_koan = pd.DataFrame(koan_units)\n",
        "\n",
        "total_tokens = df_koan['token_count'].sum()\n",
        "print(f\"\\nì´ ê³µì•ˆ: {len(df_koan)} units\")\n",
        "print(f\"ì´ í† í°: {total_tokens:,}\")\n",
        "print(f\"  - Mumonkan: {mumonkan_count} cases\")\n",
        "print(f\"  - Blue Cliff: {blue_cliff_count} cases\")\n",
        "\n",
        "# JSON ì €ì¥\n",
        "json_path = f\"{OUTPUT_PATH}/koan_level1.json\"\n",
        "df_koan.to_json(json_path, orient='records', force_ascii=False, indent=2)\n",
        "print(f\"\\nâœ… JSON ì €ì¥: {json_path}\")\n",
        "\n",
        "# CSV ì €ì¥\n",
        "csv_path = f\"{OUTPUT_PATH}/koan_level1_meta.csv\"\n",
        "df_koan.drop(columns=['text']).to_csv(csv_path, index=False)\n",
        "print(f\"âœ… CSV ì €ì¥: {csv_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 7: Level 2 ì²­í¬ ìƒì„±\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Level 2 ì²­í¬ ìƒì„±\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "CHUNK_SIZE = 1000\n",
        "OVERLAP = 0.5\n",
        "\n",
        "doc = nlp(koan_full_text)\n",
        "tokens = [t.text for t in doc if not t.is_space]\n",
        "print(f\"ì „ì²´ í† í° ìˆ˜: {len(tokens):,}\")\n",
        "\n",
        "chunks = []\n",
        "stride = int(CHUNK_SIZE * (1 - OVERLAP))\n",
        "\n",
        "for i in range(0, len(tokens) - CHUNK_SIZE + 1, stride):\n",
        "    chunk_tokens = tokens[i:i + CHUNK_SIZE]\n",
        "    chunks.append({\n",
        "        'category': 'koan',\n",
        "        'chunk_id': f'koan_chunk_{len(chunks)}',\n",
        "        'chunk_num': len(chunks),\n",
        "        'token_count': len(chunk_tokens),\n",
        "        'text': ' '.join(chunk_tokens)\n",
        "    })\n",
        "\n",
        "print(f\"ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ\")\n",
        "\n",
        "df_chunks = pd.DataFrame(chunks)\n",
        "\n",
        "json_path = f\"{CHUNKS_PATH}/koan_level2.json\"\n",
        "df_chunks.to_json(json_path, orient='records', force_ascii=False, indent=2)\n",
        "print(f\"\\nâœ… JSON ì €ì¥: {json_path}\")\n",
        "\n",
        "csv_path = f\"{CHUNKS_PATH}/koan_level2_meta.csv\"\n",
        "df_chunks.drop(columns=['text']).to_csv(csv_path, index=False)\n",
        "print(f\"âœ… CSV ì €ì¥: {csv_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 8: ìµœì¢… ìš”ì•½\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ğŸ“Š ê³µì•ˆ ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\"\"\n",
        "[Level 1 - ì›ë³¸ ë‹¨ìœ„]\n",
        "  Mumonkan:    {mumonkan_count} cases\n",
        "  Blue Cliff:  {blue_cliff_count} cases\n",
        "  ì´ê³„:        {len(koan_units)} units\n",
        "  ì´ í† í°:     {total_tokens:,}\n",
        "\n",
        "[Level 2 - ì²­í¬]\n",
        "  ì²­í¬ ìˆ˜:     {len(chunks)}ê°œ\n",
        "  ì²­í¬ í¬ê¸°:   {CHUNK_SIZE} tokens\n",
        "  ì˜¤ë²„ë©:      {int(OVERLAP*100)}%\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcvFanGUchGL",
        "outputId": "5e8e67dc-f042-482b-c5f1-346aa563507e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "í™˜ê²½ ì„¤ì • ì™„ë£Œ!\n",
            "âœ… Blue Cliff Case 100 ìˆ˜ì • ì™„ë£Œ!\n",
            "Blue Cliff ì´ ì¼€ì´ìŠ¤: 100\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "spaCy ë¡œë“œ ì™„ë£Œ!\n",
            "í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n",
            "==================================================\n",
            "ê³µì•ˆ íŒŒì‹± ì‹œì‘\n",
            "==================================================\n",
            "\n",
            "Mumonkan ë§ˆì»¤ ìˆ˜: 48\n",
            "Mumonkan íŒŒì‹± ì™„ë£Œ: 48 cases\n",
            "\n",
            "Blue Cliff ë§ˆì»¤ ìˆ˜: 100\n",
            "Blue Cliff íŒŒì‹± ì™„ë£Œ: 100 cases\n",
            "\n",
            "==================================================\n",
            "Level 1 ì €ì¥\n",
            "==================================================\n",
            "\n",
            "ì´ ê³µì•ˆ: 148 units\n",
            "ì´ í† í°: 14,431\n",
            "  - Mumonkan: 48 cases\n",
            "  - Blue Cliff: 100 cases\n",
            "\n",
            "âœ… JSON ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/processed/koan_level1.json\n",
            "âœ… CSV ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/processed/koan_level1_meta.csv\n",
            "\n",
            "==================================================\n",
            "Level 2 ì²­í¬ ìƒì„±\n",
            "==================================================\n",
            "ì „ì²´ í† í° ìˆ˜: 14,431\n",
            "ìƒì„±ëœ ì²­í¬: 27ê°œ\n",
            "\n",
            "âœ… JSON ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/chunks/koan_level2.json\n",
            "âœ… CSV ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/chunks/koan_level2_meta.csv\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š ê³µì•ˆ ì „ì²˜ë¦¬ ì™„ë£Œ\n",
            "==================================================\n",
            "\n",
            "[Level 1 - ì›ë³¸ ë‹¨ìœ„]\n",
            "  Mumonkan:    48 cases\n",
            "  Blue Cliff:  100 cases\n",
            "  ì´ê³„:        148 units\n",
            "  ì´ í† í°:     14,431\n",
            "\n",
            "[Level 2 - ì²­í¬]\n",
            "  ì²­í¬ ìˆ˜:     27ê°œ\n",
            "  ì²­í¬ í¬ê¸°:   1000 tokens\n",
            "  ì˜¤ë²„ë©:      50%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ì„  ê³µì•ˆ NLP ì—°êµ¬ - ì „ì²˜ë¦¬ Step 2.2: ë¶ˆêµ ê²½ì „ (Sutras)\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 1: í™˜ê²½ ì„¤ì •\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.max_length = 5000000\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/zen-koan-nlp-analysis'\n",
        "RAW_PATH = f'{BASE_PATH}/data/extracted_texts_v10'\n",
        "OUTPUT_PATH = f'{BASE_PATH}/data/processed'\n",
        "CHUNKS_PATH = f'{BASE_PATH}/data/chunks'\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(CHUNKS_PATH, exist_ok=True)\n",
        "\n",
        "def load_text_file(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    metadata = {}\n",
        "    lines = content.split('\\n')\n",
        "    body_start = 0\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.startswith('#'):\n",
        "            if ':' in line:\n",
        "                key, value = line[1:].split(':', 1)\n",
        "                metadata[key.strip().lower()] = value.strip()\n",
        "            body_start = i + 1\n",
        "        else:\n",
        "            break\n",
        "    body = '\\n'.join(lines[body_start:]).strip()\n",
        "    return body, metadata\n",
        "\n",
        "def tokenize_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [t for t in doc if not t.is_space]\n",
        "    sentences = list(doc.sents)\n",
        "    return len(tokens), len(sentences)\n",
        "\n",
        "print(\"í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 2: ë¶ˆêµ ê²½ì „ íŒŒì‹±\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"ë¶ˆêµ ê²½ì „ íŒŒì‹± ì‹œì‘\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "sutra_units = []\n",
        "sutra_full_text = \"\"\n",
        "\n",
        "SUTRA_FILES = [\n",
        "    ('sutras_avatamsaka.txt', 'avatamsaka', 'Bhikshu Dharmamitra'),\n",
        "    ('sutras_lankavatara.txt', 'lankavatara', 'D.T. Suzuki'),\n",
        "    ('sutras_lotus.txt', 'lotus', 'BDK'),\n",
        "    ('sutras_vimalakirti.txt', 'vimalakirti', 'Robert Thurman'),\n",
        "    ('sutras_platform.txt', 'platform', 'Philip Yampolsky'),\n",
        "    ('sutras_diamond.txt', 'diamond', 'A.F. Price'),\n",
        "    ('sutras_amitabha.txt', 'amitabha', 'Various'),\n",
        "    ('sutras_heart.txt', 'heart', 'Various'),\n",
        "]\n",
        "\n",
        "chapter_pattern = r'=== CHAPTER (\\d+) ==='\n",
        "\n",
        "for filename, name, default_translator in SUTRA_FILES:\n",
        "    filepath = f\"{RAW_PATH}/{filename}\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"  âš  {filename} ì—†ìŒ\")\n",
        "        continue\n",
        "\n",
        "    text, metadata = load_text_file(filepath)\n",
        "    translator = metadata.get('translator', default_translator)\n",
        "    sutra_full_text += text + \"\\n\\n\"\n",
        "\n",
        "    markers = list(re.finditer(chapter_pattern, text))\n",
        "\n",
        "    if markers:\n",
        "        for i, match in enumerate(markers):\n",
        "            ch_num = int(match.group(1))\n",
        "            start = match.end()\n",
        "            end = markers[i+1].start() if i+1 < len(markers) else len(text)\n",
        "            ch_text = text[start:end].strip()\n",
        "\n",
        "            if ch_text:\n",
        "                token_count, sent_count = tokenize_text(ch_text)\n",
        "                sutra_units.append({\n",
        "                    'category': 'sutra',\n",
        "                    'text_name': name,\n",
        "                    'unit_id': f'{name}_ch_{ch_num}',\n",
        "                    'unit_num': ch_num,\n",
        "                    'text': ch_text,\n",
        "                    'token_count': token_count,\n",
        "                    'sentence_count': sent_count,\n",
        "                    'translator': translator\n",
        "                })\n",
        "        print(f\"  {name}: {len(markers)} chapters\")\n",
        "    else:\n",
        "        token_count, sent_count = tokenize_text(text)\n",
        "        sutra_units.append({\n",
        "            'category': 'sutra',\n",
        "            'text_name': name,\n",
        "            'unit_id': f'{name}_full',\n",
        "            'unit_num': 1,\n",
        "            'text': text,\n",
        "            'token_count': token_count,\n",
        "            'sentence_count': sent_count,\n",
        "            'translator': translator\n",
        "        })\n",
        "        print(f\"  {name}: 1 unit (ì „ì²´)\")\n",
        "\n",
        "print(f\"\\nì´ ê²½ì „ units: {len(sutra_units)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 3: Level 1 ì €ì¥\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Level 1 ì €ì¥\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "df_sutra = pd.DataFrame(sutra_units)\n",
        "\n",
        "print(\"\\nê²½ì „ë³„ í†µê³„:\")\n",
        "for name in df_sutra['text_name'].unique():\n",
        "    subset = df_sutra[df_sutra['text_name'] == name]\n",
        "    print(f\"  {name}: {len(subset)} units, {subset['token_count'].sum():,} tokens\")\n",
        "\n",
        "total_tokens = df_sutra['token_count'].sum()\n",
        "print(f\"\\nì´ê³„: {len(df_sutra)} units, {total_tokens:,} tokens\")\n",
        "\n",
        "json_path = f\"{OUTPUT_PATH}/sutra_level1.json\"\n",
        "df_sutra.to_json(json_path, orient='records', force_ascii=False, indent=2)\n",
        "print(f\"\\nâœ… JSON ì €ì¥: {json_path}\")\n",
        "\n",
        "csv_path = f\"{OUTPUT_PATH}/sutra_level1_meta.csv\"\n",
        "df_sutra.drop(columns=['text']).to_csv(csv_path, index=False)\n",
        "print(f\"âœ… CSV ì €ì¥: {csv_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 4: Level 2 ì²­í¬ ìƒì„±\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Level 2 ì²­í¬ ìƒì„±\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "CHUNK_SIZE = 1000\n",
        "OVERLAP = 0.5\n",
        "\n",
        "print(f\"ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sutra_full_text):,} ê¸€ì\")\n",
        "\n",
        "# ë¶„í•  í† í°í™” (ë©”ëª¨ë¦¬ ë¬¸ì œ ë°©ì§€)\n",
        "tokens = []\n",
        "chunk_chars = 400000\n",
        "for start in range(0, len(sutra_full_text), chunk_chars):\n",
        "    chunk_text = sutra_full_text[start:start + chunk_chars]\n",
        "    doc = nlp(chunk_text)\n",
        "    tokens.extend([t.text for t in doc if not t.is_space])\n",
        "    print(f\"  í† í°í™” ì§„í–‰: {len(tokens):,} tokens\")\n",
        "\n",
        "print(f\"ì „ì²´ í† í° ìˆ˜: {len(tokens):,}\")\n",
        "\n",
        "chunks = []\n",
        "stride = int(CHUNK_SIZE * (1 - OVERLAP))\n",
        "\n",
        "for i in range(0, len(tokens) - CHUNK_SIZE + 1, stride):\n",
        "    chunk_tokens = tokens[i:i + CHUNK_SIZE]\n",
        "    chunks.append({\n",
        "        'category': 'sutra',\n",
        "        'chunk_id': f'sutra_chunk_{len(chunks)}',\n",
        "        'chunk_num': len(chunks),\n",
        "        'token_count': len(chunk_tokens),\n",
        "        'text': ' '.join(chunk_tokens)\n",
        "    })\n",
        "\n",
        "print(f\"ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ\")\n",
        "\n",
        "df_chunks = pd.DataFrame(chunks)\n",
        "\n",
        "json_path = f\"{CHUNKS_PATH}/sutra_level2.json\"\n",
        "df_chunks.to_json(json_path, orient='records', force_ascii=False, indent=2)\n",
        "print(f\"\\nâœ… JSON ì €ì¥: {json_path}\")\n",
        "\n",
        "csv_path = f\"{CHUNKS_PATH}/sutra_level2_meta.csv\"\n",
        "df_chunks.drop(columns=['text']).to_csv(csv_path, index=False)\n",
        "print(f\"âœ… CSV ì €ì¥: {csv_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5: ìµœì¢… ìš”ì•½\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ğŸ“Š ë¶ˆêµ ê²½ì „ ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\"\"\n",
        "[Level 1 - ì›ë³¸ ë‹¨ìœ„]\n",
        "  ì´ ê²½ì „: {len(df_sutra['text_name'].unique())}ê°œ\n",
        "  ì´ units: {len(sutra_units)}\n",
        "  ì´ í† í°: {total_tokens:,}\n",
        "\n",
        "[Level 2 - ì²­í¬]\n",
        "  ì²­í¬ ìˆ˜: {len(chunks)}ê°œ\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-kc1By4dK4w",
        "outputId": "3cb3e829-9882-4bdc-bca5-6fa3ab33719b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "í™˜ê²½ ì„¤ì • ì™„ë£Œ!\n",
            "==================================================\n",
            "ë¶ˆêµ ê²½ì „ íŒŒì‹± ì‹œì‘\n",
            "==================================================\n",
            "  avatamsaka: 39 chapters\n",
            "  lankavatara: 4 chapters\n",
            "  lotus: 28 chapters\n",
            "  vimalakirti: 12 chapters\n",
            "  platform: 10 chapters\n",
            "  diamond: 32 chapters\n",
            "  amitabha: 1 unit (ì „ì²´)\n",
            "  heart: 1 unit (ì „ì²´)\n",
            "\n",
            "ì´ ê²½ì „ units: 127\n",
            "\n",
            "==================================================\n",
            "Level 1 ì €ì¥\n",
            "==================================================\n",
            "\n",
            "ê²½ì „ë³„ í†µê³„:\n",
            "  avatamsaka: 39 units, 944,225 tokens\n",
            "  lankavatara: 4 units, 102,759 tokens\n",
            "  lotus: 28 units, 94,291 tokens\n",
            "  vimalakirti: 12 units, 45,274 tokens\n",
            "  platform: 10 units, 35,411 tokens\n",
            "  diamond: 32 units, 8,164 tokens\n",
            "  amitabha: 1 units, 2,219 tokens\n",
            "  heart: 1 units, 311 tokens\n",
            "\n",
            "ì´ê³„: 127 units, 1,232,654 tokens\n",
            "\n",
            "âœ… JSON ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/processed/sutra_level1.json\n",
            "âœ… CSV ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/processed/sutra_level1_meta.csv\n",
            "\n",
            "==================================================\n",
            "Level 2 ì²­í¬ ìƒì„±\n",
            "==================================================\n",
            "ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: 6,492,726 ê¸€ì\n",
            "  í† í°í™” ì§„í–‰: 71,418 tokens\n",
            "  í† í°í™” ì§„í–‰: 148,245 tokens\n",
            "  í† í°í™” ì§„í–‰: 223,753 tokens\n",
            "  í† í°í™” ì§„í–‰: 297,793 tokens\n",
            "  í† í°í™” ì§„í–‰: 371,034 tokens\n",
            "  í† í°í™” ì§„í–‰: 444,977 tokens\n",
            "  í† í°í™” ì§„í–‰: 520,340 tokens\n",
            "  í† í°í™” ì§„í–‰: 595,973 tokens\n",
            "  í† í°í™” ì§„í–‰: 670,332 tokens\n",
            "  í† í°í™” ì§„í–‰: 746,300 tokens\n",
            "  í† í°í™” ì§„í–‰: 822,318 tokens\n",
            "  í† í°í™” ì§„í–‰: 899,178 tokens\n",
            "  í† í°í™” ì§„í–‰: 979,122 tokens\n",
            "  í† í°í™” ì§„í–‰: 1,056,530 tokens\n",
            "  í† í°í™” ì§„í–‰: 1,134,161 tokens\n",
            "  í† í°í™” ì§„í–‰: 1,214,566 tokens\n",
            "  í† í°í™” ì§„í–‰: 1,233,666 tokens\n",
            "ì „ì²´ í† í° ìˆ˜: 1,233,666\n",
            "ìƒì„±ëœ ì²­í¬: 2466ê°œ\n",
            "\n",
            "âœ… JSON ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/chunks/sutra_level2.json\n",
            "âœ… CSV ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/chunks/sutra_level2_meta.csv\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š ë¶ˆêµ ê²½ì „ ì „ì²˜ë¦¬ ì™„ë£Œ\n",
            "==================================================\n",
            "\n",
            "[Level 1 - ì›ë³¸ ë‹¨ìœ„]\n",
            "  ì´ ê²½ì „: 8ê°œ\n",
            "  ì´ units: 127\n",
            "  ì´ í† í°: 1,232,654\n",
            "\n",
            "[Level 2 - ì²­í¬]\n",
            "  ì²­í¬ ìˆ˜: 2466ê°œ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP 2.3: ìœ êµ ê²½ì „ ì „ì²˜ë¦¬ (ì¤‘ìš© 33ì¥ ì—…ë°ì´íŠ¸)\n",
        "# ============================================\n",
        "\n",
        "import json\n",
        "import re\n",
        "import spacy\n",
        "from pathlib import Path\n",
        "\n",
        "# spaCy ë¡œë“œ\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 2000000\n",
        "\n",
        "BASE_PATH = Path(\"/content/drive/MyDrive/zen-koan-nlp-analysis\")\n",
        "RAW_PATH = BASE_PATH / \"data\" / \"extracted_texts_v10\"\n",
        "PROCESSED_PATH = BASE_PATH / \"data\" / \"processed\"\n",
        "CHUNKS_PATH = BASE_PATH / \"data\" / \"chunks\"\n",
        "\n",
        "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
        "CHUNKS_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ìœ êµ í…ìŠ¤íŠ¸ ì„¤ì •\n",
        "CONFUCIAN_TEXTS = {\n",
        "    'analects': {'file': 'confucian_analects.txt', 'translator': 'James Legge'},\n",
        "    'mencius': {'file': 'confucian_mencius.txt', 'translator': 'James Legge'},\n",
        "    'doctrine_mean': {'file': 'confucian_doctrine_mean.txt', 'translator': 'Robert Eno'},\n",
        "    'filial_piety': {'file': 'confucian_filial_piety.txt', 'translator': 'Various'},\n",
        "    'great_learning': {'file': 'confucian_great_learning.txt', 'translator': 'Robert Eno'}\n",
        "}\n",
        "\n",
        "def parse_confucian_text(text_name, content):\n",
        "    \"\"\"ìœ êµ í…ìŠ¤íŠ¸ë³„ íŒŒì‹±\"\"\"\n",
        "    units = []\n",
        "\n",
        "    if text_name == 'analects':\n",
        "        # Book ë§ˆì»¤: ^- [Name] \\d+$\n",
        "        pattern = r'^- ([A-Za-z\\s]+) (\\d+)\\s*$'\n",
        "        current_unit = None\n",
        "        current_lines = []\n",
        "\n",
        "        for line in content.split('\\n'):\n",
        "            match = re.match(pattern, line)\n",
        "            if match:\n",
        "                if current_unit and current_lines:\n",
        "                    units.append({\n",
        "                        'unit_id': f\"analects_book_{current_unit}\",\n",
        "                        'unit_num': int(current_unit),\n",
        "                        'text': '\\n'.join(current_lines).strip()\n",
        "                    })\n",
        "                current_unit = match.group(2)\n",
        "                current_lines = []\n",
        "            elif current_unit:\n",
        "                current_lines.append(line)\n",
        "\n",
        "        if current_unit and current_lines:\n",
        "            units.append({\n",
        "                'unit_id': f\"analects_book_{current_unit}\",\n",
        "                'unit_num': int(current_unit),\n",
        "                'text': '\\n'.join(current_lines).strip()\n",
        "            })\n",
        "\n",
        "    elif text_name == 'mencius':\n",
        "        # Section ë§ˆì»¤: ^- [Name]$ (ìˆ«ì ì—†ìŒ)\n",
        "        pattern = r'^- ([A-Za-z\\s]+)\\s*$'\n",
        "        current_num = 0\n",
        "        current_lines = []\n",
        "\n",
        "        for line in content.split('\\n'):\n",
        "            match = re.match(pattern, line)\n",
        "            if match:\n",
        "                if current_num > 0 and current_lines:\n",
        "                    units.append({\n",
        "                        'unit_id': f\"mencius_section_{current_num}\",\n",
        "                        'unit_num': current_num,\n",
        "                        'text': '\\n'.join(current_lines).strip()\n",
        "                    })\n",
        "                current_num += 1\n",
        "                current_lines = []\n",
        "            elif current_num > 0:\n",
        "                current_lines.append(line)\n",
        "\n",
        "        if current_num > 0 and current_lines:\n",
        "            units.append({\n",
        "                'unit_id': f\"mencius_section_{current_num}\",\n",
        "                'unit_num': current_num,\n",
        "                'text': '\\n'.join(current_lines).strip()\n",
        "            })\n",
        "\n",
        "    elif text_name == 'doctrine_mean':\n",
        "        # â˜… ì—…ë°ì´íŠ¸: === CHAPTER N === ë§ˆì»¤ (33ì¥)\n",
        "        pattern = r'=== CHAPTER (\\d+) ==='\n",
        "        parts = re.split(pattern, content)\n",
        "\n",
        "        for i in range(1, len(parts), 2):\n",
        "            ch_num = int(parts[i])\n",
        "            ch_text = parts[i + 1].strip() if i + 1 < len(parts) else \"\"\n",
        "            if ch_text:\n",
        "                units.append({\n",
        "                    'unit_id': f\"doctrine_mean_ch_{ch_num}\",\n",
        "                    'unit_num': ch_num,\n",
        "                    'text': ch_text\n",
        "                })\n",
        "\n",
        "    elif text_name == 'filial_piety':\n",
        "        # CHAPTER ë§ˆì»¤: ^CHAPTER [WORD]:\n",
        "        pattern = r'^CHAPTER ([A-Z]+):'\n",
        "        word_to_num = {\n",
        "            'ONE': 1, 'TWO': 2, 'THREE': 3, 'FOUR': 4, 'FIVE': 5,\n",
        "            'SIX': 6, 'SEVEN': 7, 'EIGHT': 8, 'NINE': 9, 'TEN': 10,\n",
        "            'ELEVEN': 11, 'TWELVE': 12, 'THIRTEEN': 13, 'FOURTEEN': 14,\n",
        "            'FIFTEEN': 15, 'SIXTEEN': 16, 'SEVENTEEN': 17, 'EIGHTEEN': 18\n",
        "        }\n",
        "        current_unit = None\n",
        "        current_lines = []\n",
        "\n",
        "        for line in content.split('\\n'):\n",
        "            match = re.match(pattern, line)\n",
        "            if match:\n",
        "                if current_unit and current_lines:\n",
        "                    units.append({\n",
        "                        'unit_id': f\"filial_piety_ch_{current_unit}\",\n",
        "                        'unit_num': current_unit,\n",
        "                        'text': '\\n'.join(current_lines).strip()\n",
        "                    })\n",
        "                word = match.group(1)\n",
        "                current_unit = word_to_num.get(word, 0)\n",
        "                current_lines = [line]\n",
        "            elif current_unit:\n",
        "                current_lines.append(line)\n",
        "\n",
        "        if current_unit and current_lines:\n",
        "            units.append({\n",
        "                'unit_id': f\"filial_piety_ch_{current_unit}\",\n",
        "                'unit_num': current_unit,\n",
        "                'text': '\\n'.join(current_lines).strip()\n",
        "            })\n",
        "\n",
        "    elif text_name == 'great_learning':\n",
        "        # ë‹¨ì¼ unit\n",
        "        clean_text = '\\n'.join([\n",
        "            line for line in content.split('\\n')\n",
        "            if line.strip() and not line.startswith('#')\n",
        "        ])\n",
        "        units.append({\n",
        "            'unit_id': 'great_learning_full',\n",
        "            'unit_num': 1,\n",
        "            'text': clean_text.strip()\n",
        "        })\n",
        "\n",
        "    return units\n",
        "\n",
        "def process_text_with_spacy(text):\n",
        "    \"\"\"spaCyë¡œ í† í°í™” ë° ë¬¸ì¥ ë¶„ë¦¬\"\"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_space]\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    return tokens, sentences\n",
        "\n",
        "def create_chunks(units, chunk_size=1000, overlap=500):\n",
        "    \"\"\"ì²­í¬ ìƒì„±\"\"\"\n",
        "    all_tokens = []\n",
        "    for unit in units:\n",
        "        all_tokens.extend(unit['tokens'])\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    chunk_num = 1\n",
        "\n",
        "    while start < len(all_tokens):\n",
        "        end = min(start + chunk_size, len(all_tokens))\n",
        "        chunk_tokens = all_tokens[start:end]\n",
        "\n",
        "        chunks.append({\n",
        "            'chunk_id': f\"chunk_{chunk_num}\",\n",
        "            'chunk_num': chunk_num,\n",
        "            'tokens': chunk_tokens,\n",
        "            'token_count': len(chunk_tokens),\n",
        "            'text': ' '.join(chunk_tokens)\n",
        "        })\n",
        "\n",
        "        chunk_num += 1\n",
        "        start += overlap\n",
        "\n",
        "        if end >= len(all_tokens):\n",
        "            break\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# ë©”ì¸ ì²˜ë¦¬\n",
        "print(\"=\" * 50)\n",
        "print(\"ìœ êµ ê²½ì „ ì „ì²˜ë¦¬ ì‹œì‘ (ì¤‘ìš© 33ì¥ ì—…ë°ì´íŠ¸)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "all_units = []\n",
        "total_tokens = 0\n",
        "\n",
        "for text_name, info in CONFUCIAN_TEXTS.items():\n",
        "    file_path = RAW_PATH / info['file']\n",
        "\n",
        "    if not file_path.exists():\n",
        "        print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {info['file']}\")\n",
        "        continue\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # íŒŒì‹±\n",
        "    units = parse_confucian_text(text_name, content)\n",
        "    print(f\"\\n{text_name}: {len(units)} units íŒŒì‹±ë¨\")\n",
        "\n",
        "    # spaCy ì²˜ë¦¬\n",
        "    for unit in units:\n",
        "        tokens, sentences = process_text_with_spacy(unit['text'])\n",
        "        unit['tokens'] = tokens\n",
        "        unit['sentences'] = sentences\n",
        "        unit['token_count'] = len(tokens)\n",
        "        unit['sentence_count'] = len(sentences)\n",
        "        unit['category'] = 'confucian'\n",
        "        unit['text_name'] = text_name\n",
        "        unit['translator'] = info['translator']\n",
        "        total_tokens += len(tokens)\n",
        "\n",
        "    all_units.extend(units)\n",
        "\n",
        "    # ê°œë³„ í†µê³„\n",
        "    unit_tokens = sum(u['token_count'] for u in units)\n",
        "    print(f\"  â†’ {len(units)} units, {unit_tokens:,} tokens\")\n",
        "\n",
        "print(f\"\\nì´ Level 1 units: {len(all_units)}\")\n",
        "print(f\"ì´ tokens: {total_tokens:,}\")\n",
        "\n",
        "# Level 2 ì²­í¬ ìƒì„±\n",
        "all_chunks = create_chunks(all_units, chunk_size=1000, overlap=500)\n",
        "for chunk in all_chunks:\n",
        "    chunk['category'] = 'confucian'\n",
        "    chunk['text_name'] = 'confucian_combined'\n",
        "\n",
        "print(f\"Level 2 chunks: {len(all_chunks)}\")\n",
        "\n",
        "# Level 1 ì €ì¥\n",
        "level1_data = []\n",
        "for unit in all_units:\n",
        "    level1_data.append({\n",
        "        'category': unit['category'],\n",
        "        'text_name': unit['text_name'],\n",
        "        'unit_id': unit['unit_id'],\n",
        "        'unit_num': unit['unit_num'],\n",
        "        'token_count': unit['token_count'],\n",
        "        'sentence_count': unit['sentence_count'],\n",
        "        'translator': unit['translator'],\n",
        "        'text': unit['text']\n",
        "    })\n",
        "\n",
        "with open(PROCESSED_PATH / 'confucian_level1.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(level1_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Level 1 ë©”íƒ€ CSV\n",
        "import csv\n",
        "with open(PROCESSED_PATH / 'confucian_level1_meta.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['category', 'text_name', 'unit_id', 'unit_num', 'token_count', 'sentence_count', 'translator'])\n",
        "    for unit in level1_data:\n",
        "        writer.writerow([\n",
        "            unit['category'], unit['text_name'], unit['unit_id'],\n",
        "            unit['unit_num'], unit['token_count'], unit['sentence_count'], unit['translator']\n",
        "        ])\n",
        "\n",
        "# Level 2 ì €ì¥\n",
        "level2_data = []\n",
        "for chunk in all_chunks:\n",
        "    level2_data.append({\n",
        "        'category': chunk['category'],\n",
        "        'text_name': chunk['text_name'],\n",
        "        'chunk_id': chunk['chunk_id'],\n",
        "        'chunk_num': chunk['chunk_num'],\n",
        "        'token_count': chunk['token_count'],\n",
        "        'text': chunk['text']\n",
        "    })\n",
        "\n",
        "with open(CHUNKS_PATH / 'confucian_level2.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(level2_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Level 2 ë©”íƒ€ CSV\n",
        "with open(CHUNKS_PATH / 'confucian_level2_meta.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['category', 'text_name', 'chunk_id', 'chunk_num', 'token_count'])\n",
        "    for chunk in level2_data:\n",
        "        writer.writerow([\n",
        "            chunk['category'], chunk['text_name'], chunk['chunk_id'],\n",
        "            chunk['chunk_num'], chunk['token_count']\n",
        "        ])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"âœ… ìœ êµ ê²½ì „ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Level 1: {len(all_units)} units â†’ confucian_level1.json\")\n",
        "print(f\"Level 2: {len(all_chunks)} chunks â†’ confucian_level2.json\")\n",
        "\n",
        "# í…ìŠ¤íŠ¸ë³„ ìš”ì•½\n",
        "print(\"\\nğŸ“Š í…ìŠ¤íŠ¸ë³„ ìš”ì•½:\")\n",
        "for text_name in CONFUCIAN_TEXTS.keys():\n",
        "    text_units = [u for u in all_units if u['text_name'] == text_name]\n",
        "    if text_units:\n",
        "        tokens = sum(u['token_count'] for u in text_units)\n",
        "        print(f\"  {text_name}: {len(text_units)} units, {tokens:,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43q9RyLwoCcI",
        "outputId": "f7959830-93f2-4b58-fbdb-14930fe5cf31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "ìœ êµ ê²½ì „ ì „ì²˜ë¦¬ ì‹œì‘ (ì¤‘ìš© 33ì¥ ì—…ë°ì´íŠ¸)\n",
            "==================================================\n",
            "\n",
            "analects: 20 units íŒŒì‹±ë¨\n",
            "  â†’ 20 units, 34,254 tokens\n",
            "\n",
            "mencius: 14 units íŒŒì‹±ë¨\n",
            "  â†’ 14 units, 72,239 tokens\n",
            "\n",
            "doctrine_mean: 33 units íŒŒì‹±ë¨\n",
            "  â†’ 33 units, 6,262 tokens\n",
            "\n",
            "filial_piety: 18 units íŒŒì‹±ë¨\n",
            "  â†’ 18 units, 3,328 tokens\n",
            "\n",
            "great_learning: 1 units íŒŒì‹±ë¨\n",
            "  â†’ 1 units, 4,069 tokens\n",
            "\n",
            "ì´ Level 1 units: 86\n",
            "ì´ tokens: 120,152\n",
            "Level 2 chunks: 240\n",
            "\n",
            "==================================================\n",
            "âœ… ìœ êµ ê²½ì „ ì „ì²˜ë¦¬ ì™„ë£Œ!\n",
            "==================================================\n",
            "Level 1: 86 units â†’ confucian_level1.json\n",
            "Level 2: 240 chunks â†’ confucian_level2.json\n",
            "\n",
            "ğŸ“Š í…ìŠ¤íŠ¸ë³„ ìš”ì•½:\n",
            "  analects: 20 units, 34,254 tokens\n",
            "  mencius: 14 units, 72,239 tokens\n",
            "  doctrine_mean: 33 units, 6,262 tokens\n",
            "  filial_piety: 18 units, 3,328 tokens\n",
            "  great_learning: 1 units, 4,069 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ì„  ê³µì•ˆ NLP ì—°êµ¬ - ì „ì²˜ë¦¬ Step 2.4: ë‹¹ì†¡ ì‚°ë¬¸ (Prose)\n",
        "- Level 1: 57í¸ ì—ì„¸ì´ ì „ì²´ (ì„ ë³„ ì—†ì´)\n",
        "- Level 2: 1000 tokens, 50% overlap ì²­í¬\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 1: í™˜ê²½ ì„¤ì •\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.max_length = 3000000\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/zen-koan-nlp-analysis'\n",
        "RAW_PATH = f'{BASE_PATH}/data/extracted_texts_v10'\n",
        "OUTPUT_PATH = f'{BASE_PATH}/data/processed'\n",
        "CHUNKS_PATH = f'{BASE_PATH}/data/chunks'\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(CHUNKS_PATH, exist_ok=True)\n",
        "\n",
        "def tokenize_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [t for t in doc if not t.is_space]\n",
        "    sentences = list(doc.sents)\n",
        "    return len(tokens), len(sentences)\n",
        "\n",
        "print(\"í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 2: ì‚°ë¬¸ íŒŒì‹±\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"ë‹¹ì†¡ ì‚°ë¬¸ íŒŒì‹± ì‹œì‘\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "prose_units = []\n",
        "prose_full_text = \"\"\n",
        "\n",
        "filepath = f\"{RAW_PATH}/prose_tang_song.txt\"\n",
        "\n",
        "with open(filepath, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "print(f\"íŒŒì¼ ë¡œë“œ: {len(content):,} ë¬¸ì\")\n",
        "\n",
        "# ì—ì„¸ì´ ë§ˆì»¤ íŒ¨í„´: === ESSAY N: TITLE (AUTHOR, ERA) ===\n",
        "essay_pattern = r'=== ESSAY (\\d+): ([^(]+) \\(([^,]+), (Tang|Song)\\) ==='\n",
        "markers = list(re.finditer(essay_pattern, content))\n",
        "print(f\"ì—ì„¸ì´ ë§ˆì»¤ ìˆ˜: {len(markers)}\")\n",
        "\n",
        "for i, match in enumerate(markers):\n",
        "    essay_num = int(match.group(1))\n",
        "    title = match.group(2).strip()\n",
        "    author = match.group(3).strip()\n",
        "    era = match.group(4).strip()\n",
        "\n",
        "    start = match.end()\n",
        "    end = markers[i+1].start() if i+1 < len(markers) else len(content)\n",
        "    essay_text = content[start:end].strip()\n",
        "\n",
        "    # ë©”íƒ€ë°ì´í„° ì¤„ ì œê±° (# Author:, # Dates:, # Era:)\n",
        "    lines = essay_text.split('\\n')\n",
        "    clean_lines = [line for line in lines if not line.startswith('#')]\n",
        "    essay_text = '\\n'.join(clean_lines).strip()\n",
        "\n",
        "    if essay_text:\n",
        "        prose_full_text += essay_text + \"\\n\\n\"\n",
        "        token_count, sent_count = tokenize_text(essay_text)\n",
        "        prose_units.append({\n",
        "            'category': 'prose',\n",
        "            'text_name': era.lower(),\n",
        "            'unit_id': f'prose_essay_{essay_num}',\n",
        "            'unit_num': essay_num,\n",
        "            'title': title,\n",
        "            'author': author,\n",
        "            'era': era,\n",
        "            'text': essay_text,\n",
        "            'token_count': token_count,\n",
        "            'sentence_count': sent_count,\n",
        "            'translator': 'Herbert Giles'\n",
        "        })\n",
        "\n",
        "tang_count = len([u for u in prose_units if u['era'] == 'Tang'])\n",
        "song_count = len([u for u in prose_units if u['era'] == 'Song'])\n",
        "print(f\"\\níŒŒì‹± ì™„ë£Œ:\")\n",
        "print(f\"  - ë‹¹(Tang): {tang_count}í¸\")\n",
        "print(f\"  - ì†¡(Song): {song_count}í¸\")\n",
        "print(f\"  - ì´ê³„: {len(prose_units)}í¸\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 3: Level 1 ì €ì¥\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Level 1 ì €ì¥\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "df_prose = pd.DataFrame(prose_units)\n",
        "\n",
        "total_tokens = df_prose['token_count'].sum()\n",
        "print(f\"\\nì´ ì—ì„¸ì´: {len(df_prose)}\")\n",
        "print(f\"ì´ í† í°: {total_tokens:,}\")\n",
        "\n",
        "# ì‹œëŒ€ë³„ í†µê³„\n",
        "print(\"\\nì‹œëŒ€ë³„ í†µê³„:\")\n",
        "for era in ['Tang', 'Song']:\n",
        "    subset = df_prose[df_prose['era'] == era]\n",
        "    print(f\"  {era}: {len(subset)}í¸, {subset['token_count'].sum():,} tokens\")\n",
        "\n",
        "# ì €ìë³„ í†µê³„\n",
        "print(\"\\nì €ìë³„ ìƒìœ„ 10ëª…:\")\n",
        "author_stats = df_prose.groupby('author').agg({\n",
        "    'unit_num': 'count',\n",
        "    'token_count': 'sum'\n",
        "}).rename(columns={'unit_num': 'essays'}).sort_values('essays', ascending=False)\n",
        "for author, row in author_stats.head(10).iterrows():\n",
        "    print(f\"  {author}: {int(row['essays'])}í¸, {int(row['token_count']):,} tokens\")\n",
        "\n",
        "# JSON ì €ì¥\n",
        "json_path = f\"{OUTPUT_PATH}/prose_level1.json\"\n",
        "df_prose.to_json(json_path, orient='records', force_ascii=False, indent=2)\n",
        "print(f\"\\nâœ… JSON ì €ì¥: {json_path}\")\n",
        "\n",
        "# CSV ì €ì¥ (text ì œì™¸)\n",
        "csv_path = f\"{OUTPUT_PATH}/prose_level1_meta.csv\"\n",
        "df_prose.drop(columns=['text']).to_csv(csv_path, index=False)\n",
        "print(f\"âœ… CSV ì €ì¥: {csv_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 4: Level 2 ì²­í¬ ìƒì„±\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Level 2 ì²­í¬ ìƒì„±\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "CHUNK_SIZE = 1000\n",
        "OVERLAP = 0.5\n",
        "\n",
        "doc = nlp(prose_full_text)\n",
        "tokens = [t.text for t in doc if not t.is_space]\n",
        "print(f\"ì „ì²´ í† í° ìˆ˜: {len(tokens):,}\")\n",
        "\n",
        "chunks = []\n",
        "stride = int(CHUNK_SIZE * (1 - OVERLAP))\n",
        "\n",
        "for i in range(0, len(tokens) - CHUNK_SIZE + 1, stride):\n",
        "    chunk_tokens = tokens[i:i + CHUNK_SIZE]\n",
        "    chunks.append({\n",
        "        'category': 'prose',\n",
        "        'chunk_id': f'prose_chunk_{len(chunks)}',\n",
        "        'chunk_num': len(chunks),\n",
        "        'token_count': len(chunk_tokens),\n",
        "        'text': ' '.join(chunk_tokens)\n",
        "    })\n",
        "\n",
        "print(f\"ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ\")\n",
        "\n",
        "df_chunks = pd.DataFrame(chunks)\n",
        "\n",
        "json_path = f\"{CHUNKS_PATH}/prose_level2.json\"\n",
        "df_chunks.to_json(json_path, orient='records', force_ascii=False, indent=2)\n",
        "print(f\"\\nâœ… JSON ì €ì¥: {json_path}\")\n",
        "\n",
        "csv_path = f\"{CHUNKS_PATH}/prose_level2_meta.csv\"\n",
        "df_chunks.drop(columns=['text']).to_csv(csv_path, index=False)\n",
        "print(f\"âœ… CSV ì €ì¥: {csv_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5: ìµœì¢… ìš”ì•½\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ğŸ“Š ë‹¹ì†¡ ì‚°ë¬¸ ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\"\"\n",
        "[Level 1 - ì›ë³¸ ë‹¨ìœ„]\n",
        "  ë‹¹(Tang):    {tang_count}í¸\n",
        "  ì†¡(Song):    {song_count}í¸\n",
        "  ì´ê³„:        {len(prose_units)} essays\n",
        "  ì´ í† í°:     {total_tokens:,}\n",
        "\n",
        "[Level 2 - ì²­í¬]\n",
        "  ì²­í¬ ìˆ˜:     {len(chunks)}ê°œ\n",
        "  ì²­í¬ í¬ê¸°:   {CHUNK_SIZE} tokens\n",
        "  ì˜¤ë²„ë©:      {int(OVERLAP*100)}%\n",
        "\n",
        "âš ï¸ 25í¸ ì„ ë³„ì€ ë¶„ì„ ë‹¨ê³„ì—ì„œ ì§„í–‰\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aH7mVrU9F5Y",
        "outputId": "274f0c08-969f-4a89-b16d-50ccbf5711d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "í™˜ê²½ ì„¤ì • ì™„ë£Œ!\n",
            "==================================================\n",
            "ë‹¹ì†¡ ì‚°ë¬¸ íŒŒì‹± ì‹œì‘\n",
            "==================================================\n",
            "íŒŒì¼ ë¡œë“œ: 168,047 ë¬¸ì\n",
            "ì—ì„¸ì´ ë§ˆì»¤ ìˆ˜: 57\n",
            "\n",
            "íŒŒì‹± ì™„ë£Œ:\n",
            "  - ë‹¹(Tang): 19í¸\n",
            "  - ì†¡(Song): 38í¸\n",
            "  - ì´ê³„: 57í¸\n",
            "\n",
            "==================================================\n",
            "Level 1 ì €ì¥\n",
            "==================================================\n",
            "\n",
            "ì´ ì—ì„¸ì´: 57\n",
            "ì´ í† í°: 33,950\n",
            "\n",
            "ì‹œëŒ€ë³„ í†µê³„:\n",
            "  Tang: 19í¸, 14,817 tokens\n",
            "  Song: 38í¸, 19,133 tokens\n",
            "\n",
            "ì €ìë³„ ìƒìœ„ 10ëª…:\n",
            "  SU TUNG-P'O: 11í¸, 7,364 tokens\n",
            "  HAN WEN-KUNG: 8í¸, 7,695 tokens\n",
            "  OU-YANG HSIU: 7í¸, 4,435 tokens\n",
            "  LIU TSUNG-YUAN: 6í¸, 4,133 tokens\n",
            "  SHEN KUA: 3í¸, 784 tokens\n",
            "  CHU HSI: 2í¸, 1,216 tokens\n",
            "  SUNG TZ'U: 2í¸, 339 tokens\n",
            "  LO KUAN-CHUNG: 2í¸, 1,743 tokens\n",
            "  WANG AN-SHIH: 2í¸, 592 tokens\n",
            "  LI CHIH: 1í¸, 421 tokens\n",
            "\n",
            "âœ… JSON ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/processed/prose_level1.json\n",
            "âœ… CSV ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/processed/prose_level1_meta.csv\n",
            "\n",
            "==================================================\n",
            "Level 2 ì²­í¬ ìƒì„±\n",
            "==================================================\n",
            "ì „ì²´ í† í° ìˆ˜: 33,950\n",
            "ìƒì„±ëœ ì²­í¬: 66ê°œ\n",
            "\n",
            "âœ… JSON ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/chunks/prose_level2.json\n",
            "âœ… CSV ì €ì¥: /content/drive/MyDrive/zen-koan-nlp-analysis/data/chunks/prose_level2_meta.csv\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š ë‹¹ì†¡ ì‚°ë¬¸ ì „ì²˜ë¦¬ ì™„ë£Œ\n",
            "==================================================\n",
            "\n",
            "[Level 1 - ì›ë³¸ ë‹¨ìœ„]\n",
            "  ë‹¹(Tang):    19í¸\n",
            "  ì†¡(Song):    38í¸\n",
            "  ì´ê³„:        57 essays\n",
            "  ì´ í† í°:     33,950\n",
            "\n",
            "[Level 2 - ì²­í¬]\n",
            "  ì²­í¬ ìˆ˜:     66ê°œ\n",
            "  ì²­í¬ í¬ê¸°:   1000 tokens\n",
            "  ì˜¤ë²„ë©:      50%\n",
            "\n",
            "âš ï¸ 25í¸ ì„ ë³„ì€ ë¶„ì„ ë‹¨ê³„ì—ì„œ ì§„í–‰\n",
            "\n"
          ]
        }
      ]
    }
  ]
}