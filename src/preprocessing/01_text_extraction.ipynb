{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcVuMwuKFf6D7/sZCY2SMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyunmookKim/zen-koan-nlp-analysis/blob/master/src/preprocessing/01_text_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ì„  ê³µì•ˆ NLP ì—°êµ¬ - í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
        "# ============================================\n",
        "\n",
        "!pip install pymupdf -q\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import fitz\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/raw\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/processed\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def remove_control_chars(text):\n",
        "    return re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]', '', text)\n",
        "\n",
        "def remove_all_chinese(text):\n",
        "    patterns = [r'[\\u4e00-\\u9fff]', r'[\\u3400-\\u4dbf]', r'[\\u2e80-\\u2eff]', r'[\\u2f00-\\u2fdf]', r'[\\u3000-\\u303f]', r'[\\uff00-\\uffef]', r'[\\u31c0-\\u31ef]', r'[\\uf900-\\ufaff]']\n",
        "    for p in patterns:\n",
        "        text = re.sub(p, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_page_numbers(text):\n",
        "    lines = text.split('\\n')\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        if re.match(r'^[0-9]+[a-c]?$', stripped):\n",
        "            continue\n",
        "        if re.match(r'^[ivxlcdm]+$', stripped.lower()) and len(stripped) < 10:\n",
        "            continue\n",
        "        cleaned.append(line)\n",
        "    return '\\n'.join(cleaned)\n",
        "\n",
        "def remove_running_headers(text, headers):\n",
        "    lines = text.split('\\n')\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        skip = False\n",
        "        for h in headers:\n",
        "            if stripped == h or stripped.lower() == h.lower():\n",
        "                skip = True\n",
        "                break\n",
        "        if not skip:\n",
        "            cleaned.append(line)\n",
        "    return '\\n'.join(cleaned)\n",
        "\n",
        "def auto_detect_running_headers(text, threshold=10):\n",
        "    lines = text.split('\\n')\n",
        "    line_counts = Counter(line.strip() for line in lines if line.strip())\n",
        "    headers = []\n",
        "    for line, count in line_counts.items():\n",
        "        if count >= threshold and 3 < len(line) < 60:\n",
        "            if not re.match(r'^[0-9]+$', line):\n",
        "                headers.append(line)\n",
        "    return headers\n",
        "\n",
        "def remove_ctext_artifacts(text):\n",
        "    \"\"\"Chinese Text Project ì›¹ì‚¬ì´íŠ¸ ë¬¸êµ¬ ì œê±°\"\"\"\n",
        "    # ì¤„ ë‹¨ìœ„ ì œê±°\n",
        "    lines = text.split('\\n')\n",
        "    cleaned = []\n",
        "    skip_patterns = [\n",
        "        r'^\\[Frequencies\\]',\n",
        "        r'^\\[Text tools\\]',\n",
        "        r'^Library$',\n",
        "        r'^Resources$',\n",
        "        r'^English translation:',\n",
        "        r'^James\\s*$',\n",
        "        r'^:$',\n",
        "        r'^\\[.*\\]$',\n",
        "    ]\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        skip = False\n",
        "        for p in skip_patterns:\n",
        "            if re.match(p, stripped):\n",
        "                skip = True\n",
        "                break\n",
        "        if not skip:\n",
        "            cleaned.append(line)\n",
        "    text = '\\n'.join(cleaned)\n",
        "\n",
        "    # ì¶”ê°€: \"James Legge [?]\" ë° \"Legge [?]\" ì œê±°\n",
        "    text = re.sub(r'James\\s+Legge\\s*\\[\\?\\]', '', text)\n",
        "    text = re.sub(r'Legge\\s*\\[\\?\\]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "    text = re.sub(r'[^\\S\\n]+', ' ', text)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    lines = [line.strip() for line in text.split('\\n')]\n",
        "    return '\\n'.join(lines).strip()\n",
        "\n",
        "def full_clean(text, remove_chinese=False, remove_ctext=False, extra_headers=None):\n",
        "    text = remove_control_chars(text)\n",
        "    if remove_chinese:\n",
        "        text = remove_all_chinese(text)\n",
        "    if remove_ctext:\n",
        "        text = remove_ctext_artifacts(text)\n",
        "    text = remove_page_numbers(text)\n",
        "    auto_headers = auto_detect_running_headers(text)\n",
        "    all_headers = auto_headers + (extra_headers or [])\n",
        "    if all_headers:\n",
        "        text = remove_running_headers(text, all_headers)\n",
        "    return clean_text(text)\n",
        "\n",
        "def extract_pdf_full(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text() + \"\\n\"\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "def extract_pdf_pages(pdf_path, start_page, end_page=None):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    end = end_page if end_page else len(doc)\n",
        "    text = \"\"\n",
        "    for i in range(start_page, min(end, len(doc))):\n",
        "        text += doc[i].get_text() + \"\\n\"\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "def find_pdf(folder, keyword):\n",
        "    folder_path = os.path.join(BASE_DIR, folder)\n",
        "    if not os.path.exists(folder_path):\n",
        "        return None\n",
        "    for f in os.listdir(folder_path):\n",
        "        if keyword.lower() in f.lower() and f.endswith('.pdf'):\n",
        "            return os.path.join(folder_path, f)\n",
        "    return None\n",
        "\n",
        "def save_result(text, filename, source_info):\n",
        "    if not text or len(text.strip()) < 100:\n",
        "        print(f\"  âŒ {filename}: ë‚´ìš© ì—†ìŒ\")\n",
        "        return None\n",
        "    path = os.path.join(OUTPUT_DIR, filename)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "    words = len(text.split())\n",
        "    print(f\"  âœ… {filename}: {words:,} words\")\n",
        "    return {\"file\": filename, \"words\": words, \"source\": source_info}\n",
        "\n",
        "# ============================================\n",
        "# 1. ê³µì•ˆ\n",
        "# ============================================\n",
        "\n",
        "def extract_mumonkan():\n",
        "    print(\"\\nğŸ“– [1/15] ë¬´ë¬¸ê´€\")\n",
        "    pdf_path = find_pdf(\"koans\", \"mumonkan\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    text = remove_control_chars(text)\n",
        "    text = text.replace('\\u2019', \"'\").replace('\\u2018', \"'\")\n",
        "    pattern = r\"THE CASE\\s*\\n(.*?)(?=WU-MEN'S COMMENT)\"\n",
        "    matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "    result = \"\"\n",
        "    for i, case in enumerate(matches, 1):\n",
        "        c = case\n",
        "        c = re.sub(r'PLATE \\d+:.*?(?=\\n)', '', c)\n",
        "        c = re.sub(r'Courtesy of.*?(?=\\n)', '', c)\n",
        "        c = re.sub(r'Trans\\. by.*?(?=\\n)', '', c)\n",
        "        c = clean_text(c)\n",
        "        if c and len(c.strip()) > 20:\n",
        "            result += f\"\\n=== CASE {i} ===\\n{c.strip()}\\n\"\n",
        "    return save_result(full_clean(result), \"koans_mumonkan.txt\", \"Mumonkan, Robert Aitken\")\n",
        "\n",
        "def extract_blue_cliff():\n",
        "    print(\"\\nğŸ“– [2/15] ë²½ì•”ë¡\")\n",
        "    pdf_path = find_pdf(\"koans\", \"blue cliff\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    text = remove_control_chars(text)\n",
        "    pattern = r'\\nCASE\\s*\\n(.*?)(?=\\nNOTES\\s*\\n)'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    result = \"\"\n",
        "    for i, case in enumerate(matches, 1):\n",
        "        c = full_clean(case, extra_headers=['THE BLUE CLIFF RECORD'])\n",
        "        if c and len(c.strip()) > 30:\n",
        "            result += f\"\\n=== CASE {i} ===\\n{c.strip()}\\n\"\n",
        "    return save_result(clean_text(result), \"koans_blue_cliff.txt\", \"Blue Cliff Record, Thomas Cleary\")\n",
        "\n",
        "# ============================================\n",
        "# 2. ë¶ˆêµ ê²½ì „\n",
        "# ============================================\n",
        "\n",
        "def extract_diamond():\n",
        "    print(\"\\nğŸ“– [3/15] ê¸ˆê°•ê²½\")\n",
        "    pdf_path = find_pdf(\"sutras\", \"diamond\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    m = re.search(r'Chapter\\s*1\\.?\\s', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[m.start():]\n",
        "    m = re.search(r'(bowed to him and departed|prostrated themselves before the Buddha)', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[:m.end()]\n",
        "    return save_result(full_clean(text, extra_headers=['THE DIAMOND SUTRA']), \"sutras_diamond.txt\", \"Diamond Sutra\")\n",
        "\n",
        "def extract_heart():\n",
        "    print(\"\\nğŸ“– [4/15] ë°˜ì•¼ì‹¬ê²½\")\n",
        "    pdf_path = find_pdf(\"sutras\", \"heart\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    for p in [r'The Heart Sutra in Sanskrit', r'pronunciation guide', r'SANSKRIT']:\n",
        "        m = re.search(p, text, re.IGNORECASE)\n",
        "        if m:\n",
        "            text = text[:m.start()]\n",
        "            break\n",
        "    return save_result(full_clean(text), \"sutras_heart.txt\", \"Heart Sutra\")\n",
        "\n",
        "def extract_platform():\n",
        "    print(\"\\nğŸ“– [5/15] ìœ¡ì¡°ë‹¨ê²½\")\n",
        "    pdf_path = find_pdf(\"sutras\", \"platform\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_pages(pdf_path, 34)\n",
        "    m = re.search(r'End of The Platform Sutra.*?Sixth Patriarch\\.?', text, re.IGNORECASE | re.DOTALL)\n",
        "    if m:\n",
        "        text = text[:m.end()]\n",
        "    text = re.sub(r'\\n36[0-9][a-c]\\n', '\\n', text)\n",
        "    return save_result(full_clean(text, extra_headers=['The Platform Sutra of the Sixth Patriarch', 'Platform Sutra of the Dharma Treasure']), \"sutras_platform.txt\", \"Platform Sutra, BDK\")\n",
        "\n",
        "def extract_lotus():\n",
        "    print(\"\\nğŸ“– [6/15] ë²•í™”ê²½\")\n",
        "    pdf_path = find_pdf(\"sutras\", \"lotus\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_pages(pdf_path, 18)\n",
        "    m = re.search(r'bowed to him and departed\\.?', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[:m.end()]\n",
        "    return save_result(full_clean(text), \"sutras_lotus.txt\", \"Lotus Sutra, BDK\")\n",
        "\n",
        "def extract_lankavatara():\n",
        "    print(\"\\nğŸ“– [7/15] ëŠ¥ê°€ê²½\")\n",
        "    pdf_path = find_pdf(\"sutras\", \"lankavatara\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_pages(pdf_path, 35)\n",
        "    m = re.search(r'This marks the end of the Lankavatara Sutra\\.?', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[:m.end()]\n",
        "    return save_result(full_clean(text), \"sutras_lankavatara.txt\", \"Lankavatara Sutra, Red Pine\")\n",
        "\n",
        "def extract_vimalakirti():\n",
        "    print(\"\\nğŸ“– [8/15] ìœ ë§ˆê²½\")\n",
        "    pdf_path = find_pdf(\"sutras\", \"vimalakirti\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    m = re.search(r'Thus have I heard', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[m.start():]\n",
        "    return save_result(full_clean(text), \"sutras_vimalakirti.txt\", \"Vimalakirti Sutra, Robert Thurman\")\n",
        "\n",
        "def extract_avatamsaka():\n",
        "    print(\"\\nğŸ“– [9/15] í™”ì—„ê²½\")\n",
        "    pdf_path = find_pdf(\"sutras\", \"avatamsaka\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    m = re.search(r'Chapter One[:\\s]|CHAPTER ONE', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[m.start():]\n",
        "    m = re.search(r'\\nEndnotes\\s*\\n', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[:m.start()]\n",
        "    return save_result(full_clean(text, extra_headers=['The Flower Adornment Sutra']), \"sutras_avatamsaka.txt\", \"Avatamsaka Sutra, BDK\")\n",
        "\n",
        "def extract_amitabha():\n",
        "    print(\"\\nğŸ“– [10/15] ì•„ë¯¸íƒ€ê²½\")\n",
        "    pdf_path = find_pdf(\"sutras\", \"amitabha\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    m = re.search(r'Thus I have heard', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[m.start():]\n",
        "    return save_result(full_clean(text), \"sutras_amitabha.txt\", \"Amitabha Sutra\")\n",
        "\n",
        "# ============================================\n",
        "# 3. ìœ êµ ê²½ì „\n",
        "# ============================================\n",
        "\n",
        "def extract_analects():\n",
        "    print(\"\\nğŸ“– [11/15] ë…¼ì–´\")\n",
        "    pdf_path = find_pdf(\"confucian\", \"analects\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    return save_result(full_clean(text, remove_chinese=True, remove_ctext=True), \"confucian_analects.txt\", \"Analects, James Legge\")\n",
        "\n",
        "def extract_mencius():\n",
        "    print(\"\\nğŸ“– [12/15] ë§¹ì\")\n",
        "    pdf_path = find_pdf(\"confucian\", \"mencius\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    return save_result(full_clean(text, remove_chinese=True, remove_ctext=True), \"confucian_mencius.txt\", \"Mencius, James Legge\")\n",
        "\n",
        "def extract_doctrine():\n",
        "    print(\"\\nğŸ“– [13/15] ì¤‘ìš©Â·ëŒ€í•™\")\n",
        "    pdf_path = find_pdf(\"confucian\", \"doctrine\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    m = re.search(r'I\\.?\\s*TEXT', text)\n",
        "    if m:\n",
        "        text = text[m.start():]\n",
        "    m = re.search(r'That is the ultimate!', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[:m.end()]\n",
        "    return save_result(full_clean(text, remove_chinese=True, extra_headers=['The Great Learning', 'The Doctrine of the Mean']), \"confucian_doctrine_mean.txt\", \"Doctrine of Mean + Great Learning\")\n",
        "\n",
        "def extract_filial():\n",
        "    print(\"\\nğŸ“– [14/15] íš¨ê²½\")\n",
        "    pdf_path = find_pdf(\"confucian\", \"filial\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    m = re.search(r'disaster is sure to follow\\.', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[:m.end()]\n",
        "    text = re.sub(r'Primary Source Document.*?DBQ[s]?\\)?', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
        "    text = re.sub(r'Asia for Educators.*?reserved\\.?', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
        "    text = re.sub(r'http://afe\\.easia\\.columbia\\.edu', '', text)\n",
        "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
        "    text = re.sub(r'Columbia University', '', text)\n",
        "    return save_result(full_clean(text, remove_chinese=True), \"confucian_filial_piety.txt\", \"Classic of Filiality\")\n",
        "\n",
        "# ============================================\n",
        "# 4. ì¼ë°˜ ì‚°ë¬¸\n",
        "# ============================================\n",
        "\n",
        "def extract_prose():\n",
        "    print(\"\\nğŸ“– [15/15] ë‹¹ì†¡ ì‚°ë¬¸\")\n",
        "    pdf_path = find_pdf(\"prose\", \"gems\")\n",
        "    if not pdf_path:\n",
        "        return None\n",
        "    text = extract_pdf_full(pdf_path)\n",
        "    m = re.search(r'THE DUKE OF CHOU', text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[m.start():]\n",
        "\n",
        "    # ë§ˆì§€ë§‰ \"THE END.\" ì°¾ê¸° (INDEX ë°”ë¡œ ì•)\n",
        "    matches = list(re.finditer(r'THE\\s+END\\.', text, re.IGNORECASE))\n",
        "    if matches:\n",
        "        last_end = matches[-1]  # ë§ˆì§€ë§‰ ë§¤ì¹­\n",
        "        text = text[:last_end.end()]\n",
        "\n",
        "    return save_result(full_clean(text), \"prose_gems_chinese.txt\", \"Gems of Chinese Literature, Herbert Giles 1923\")\n",
        "\n",
        "# ============================================\n",
        "# ë©”ì¸ ì‹¤í–‰\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ğŸ“š PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ v7.0\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = []\n",
        "results.append(extract_mumonkan())\n",
        "results.append(extract_blue_cliff())\n",
        "results.append(extract_diamond())\n",
        "results.append(extract_heart())\n",
        "results.append(extract_platform())\n",
        "results.append(extract_lotus())\n",
        "results.append(extract_lankavatara())\n",
        "results.append(extract_vimalakirti())\n",
        "results.append(extract_avatamsaka())\n",
        "results.append(extract_amitabha())\n",
        "results.append(extract_analects())\n",
        "results.append(extract_mencius())\n",
        "results.append(extract_doctrine())\n",
        "results.append(extract_filial())\n",
        "results.append(extract_prose())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "success = [r for r in results if r]\n",
        "total_words = sum(r['words'] for r in success)\n",
        "print(f\"âœ… ì„±ê³µ: {len(success)}ê°œ / ì´ {total_words:,} words\")\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"extraction_log.json\"), 'w', encoding='utf-8') as f:\n",
        "    json.dump({\"version\": \"7.0\", \"date\": datetime.now().isoformat(), \"results\": success, \"total_words\": total_words}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nì™„ë£Œ: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJKd-ascTS3Q",
        "outputId": "733958bb-de80-4b88-ec42-5bd65a596d95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "============================================================\n",
            "ğŸ“š PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ v7.0\n",
            "============================================================\n",
            "\n",
            "ğŸ“– [1/15] ë¬´ë¬¸ê´€\n",
            "  âœ… koans_mumonkan.txt: 4,797 words\n",
            "\n",
            "ğŸ“– [2/15] ë²½ì•”ë¡\n",
            "  âœ… koans_blue_cliff.txt: 7,579 words\n",
            "\n",
            "ğŸ“– [3/15] ê¸ˆê°•ê²½\n",
            "  âœ… sutras_diamond.txt: 7,724 words\n",
            "\n",
            "ğŸ“– [4/15] ë°˜ì•¼ì‹¬ê²½\n",
            "  âœ… sutras_heart.txt: 264 words\n",
            "\n",
            "ğŸ“– [5/15] ìœ¡ì¡°ë‹¨ê²½\n",
            "  âœ… sutras_platform.txt: 29,212 words\n",
            "\n",
            "ğŸ“– [6/15] ë²•í™”ê²½\n",
            "  âœ… sutras_lotus.txt: 81,524 words\n",
            "\n",
            "ğŸ“– [7/15] ëŠ¥ê°€ê²½\n",
            "  âœ… sutras_lankavatara.txt: 90,272 words\n",
            "\n",
            "ğŸ“– [8/15] ìœ ë§ˆê²½\n",
            "  âœ… sutras_vimalakirti.txt: 36,425 words\n",
            "\n",
            "ğŸ“– [9/15] í™”ì—„ê²½\n",
            "  âœ… sutras_avatamsaka.txt: 258,501 words\n",
            "\n",
            "ğŸ“– [10/15] ì•„ë¯¸íƒ€ê²½\n",
            "  âœ… sutras_amitabha.txt: 1,829 words\n",
            "\n",
            "ğŸ“– [11/15] ë…¼ì–´\n",
            "  âœ… confucian_analects.txt: 28,619 words\n",
            "\n",
            "ğŸ“– [12/15] ë§¹ì\n",
            "  âœ… confucian_mencius.txt: 60,474 words\n",
            "\n",
            "ğŸ“– [13/15] ì¤‘ìš©Â·ëŒ€í•™\n",
            "  âœ… confucian_doctrine_mean.txt: 12,560 words\n",
            "\n",
            "ğŸ“– [14/15] íš¨ê²½\n",
            "  âœ… confucian_filial_piety.txt: 859 words\n",
            "\n",
            "ğŸ“– [15/15] ë‹¹ì†¡ ì‚°ë¬¸\n",
            "  âœ… prose_gems_chinese.txt: 98,578 words\n",
            "\n",
            "============================================================\n",
            "âœ… ì„±ê³µ: 15ê°œ / ì´ 719,217 words\n",
            "\n",
            "ì™„ë£Œ: /content/drive/MyDrive/processed\n"
          ]
        }
      ]
    }
  ]
}